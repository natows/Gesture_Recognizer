# â€Sterowanie komputerem przy pomocy gestÃ³w dÅ‚oni w czasie rzeczywistym â€“ porÃ³wnanie klasyfikatorÃ³w ML i prototyp aplikacji uÅ¼ytkowejâ€

## Etap pierwszy - klasyfikacja danych liczbowych

Przygotowywanie wÅ‚asnych danych dla klasyfikatorÃ³w kNN, DTC oraz MLP.
Wykorzystano paczkÄ™ media pipe i moduÅ‚ Hands do ekstrakcji cech z obrazu (kod w pliku ./creating_data.py) - jeden zapisany gest to 21 wspÃ³Å‚rzÄ™dnych uÅ‚oÅ¼enia dÅ‚oni i label danego gestu (plik ./data_sources/gestures.csv) 
Biblioteka mediapipe automatycznie normalizuje dane (wspÃ³Å‚rzÄ™dne) do przedziaÅ‚u [0,1]
Zebrano 4 nastÄ™pujÄ…ce gesty - 'palm', 'l','V' oraz 'ok'
W datasecie kaÅ¼dy z gestÃ³w ma po ok. 450 danych.

### Utworzenie prostego modelu kNN 

Liczba sÄ…siadÃ³w = 5

- Accuracy wynosi 99 %
- Przy sprawdzeniu metodÄ… cross-validation accuracy wynosi 98%. Cross-validation pokazuje dobre wyniki we wszystkich foldach, bez duÅ¼ych wahaÅ„: Wyniki cross-validation: [0.99326599 0.98989899 0.95608108 0.98648649 0.99662162]. Wskazuje to na brak przeuczenia a poprostu dobrÄ… skutecznoÅ›Ä‡ modelu.
- Wyniki classification report:
Classification Report:
              precision    recall  f1-score   support

           L       1.00      1.00      1.00       169
           V       1.00      1.00      1.00       132
          ok       1.00      0.99      1.00       117
        palm       0.99      1.00      1.00       137

    accuracy                           1.00       555
   macro avg       1.00      1.00      1.00       555
weighted avg       1.00      1.00      1.00       555

Tak wysoka dokÅ‚adnoÅ›Ä‡ moÅ¼e wynikaÄ‡ z czystoÅ›ci danych. 
- W praktyce kNN radzi sobie na ogÃ³Å‚ dobrze z identyfikacjÄ… gestu, na 20 prÃ³b 18 gestÃ³w rozpoznaÅ‚ poprawnie. ZdaÅ¼a mu siÄ™ pomyliÄ‡ 'palm' z 'ok'.
![Macierz pomyÅ‚ek dla kNN](./classifiers/kNN/knn_confusion_matrix.png)

### Utworzenie prostego modelu DTC 
- Model osiÄ…ga accuracy 99%
- Wyniki cross-validation: [0.98058252 0.98701299 0.99675325 0.98376623 0.96103896]. Tu rÃ³wnieÅ¼ bez wachaÅ„ miÄ™dzy foldami.
- Classification report:
Classification Report:
              precision    recall  f1-score   support

           L       1.00      1.00      1.00       142
           V       1.00      1.00      1.00       142
          ok       1.00      0.99      1.00       132
        palm       0.99      1.00      1.00       139

    accuracy                           1.00       555
   macro avg       1.00      1.00      1.00       555
weighted avg       1.00      1.00      1.00       555

- W praktyce model jest prawie bezbÅ‚Ä™dny, na 20 testowych prÃ³b rozpoznaÅ‚ poprawnie wszystkie 4 gesty.
![Macierz pomyÅ‚ek DTC](./classifiers/DTC/dtc_confusion_matrix.png)

### Utworzenie sieci MLP 

2 warstwy ukryte, jedna 100 druga 50 neuronÃ³w, 500 iteracji

- SieÄ‡ neuronowa osiÄ…ga accuracy 99,8%
- Wyniki cross walidacji: [0.98058252 1.         0.99675325 1.         0.99675325], brak przeuczenia
- Wyniki classification report:
Classification Report:
              precision    recall  f1-score   support

           L       1.00      1.00      1.00       142
           V       1.00      1.00      1.00       142
          ok       1.00      0.99      1.00       132
        palm       0.99      1.00      1.00       139

    accuracy                           1.00       555
   macro avg       1.00      1.00      1.00       555
weighted avg       1.00      1.00      1.00       555

- Przy 20 prÃ³bach model rozpoznaje 17/20 gestÃ³w, myli palm z V.
![Macierz pomyÅ‚ek MLP](./classifiers/MLP/mlp_confusion_matrix.png)


## Etap drugi - klasyfikacja zdjÄ™Ä‡ za pomocÄ… wÅ‚asnej konwolucyjnej sieci neuronowej

Dane zebrano znÃ³w za pomocÄ… paczki media pipe i moduÅ‚u Hands, tym razem zapisywano zdjÄ™cie wykrytej dÅ‚oni do wyznaczonego folderu (./dataset) (kod w pliku ./creating_photos.py)
ZdjÄ™cia gestÃ³w pobierane sÄ… w rozdzielczoÅ›ci **64Ã—64 pikseli** nastÄ™pujÄ…cym procesem:

1. **Detekcja dÅ‚oni** - MediaPipe wyznacza landmarks
2. **Bounding box** - wyznaczenie prostokÄ…ta otaczajÄ…cego dÅ‚oÅ„ + margines 20px
3. **Wycinanie** - ekstrakcja fragmentu obrazu z dÅ‚oniÄ…
4. **Resize** - normalizacja do 64Ã—64x3 
5. **Zapis** - format JPG na dysku

Tutaj mamy nastÄ™pujÄ…ce gesty: 'V', 'fist', 'palm', 'thumb', 'ok', 'l' 

**Preprocessing obrazÃ³w:**
- Resize do 64Ã—64 pikseli 
- Konwersja BGRâ†’RGB
- Normalizacja [0-255]â†’[0-1] 

**Preprocessing etykiet:**
- Mapowanie nazw gestÃ³w miÄ™dzy datasetami
- Label encoding stringâ†’int dla CNN
(uÅ¼ywam sparse_cathegorical_crossentropy, wiÄ™c nie musze ohehot kodowac, to robi to samo?)

**Inteligentny podziaÅ‚ danych:**
- Kaggle: podziaÅ‚ po osobach nie po obrazkach
- WÅ‚asne: stratified split
- Test generalizacji na nowych uÅ¼ytkownikach

**Data augmentation:**
- Rotacja: Â±15Â°
- PrzesuniÄ™cia: Â±10%  
- Zoom: Â±10%
- Zmiana jasnoÅ›ci: 80-120%
- Brak horizontal_flip (nie wszystkie gesty sa symetryczne np. thumb)

Architektura sieci konwolucyjnej

| Warstwa              | Typ                  | Parametry                     | WyjÅ›cie (Output Shape) |
|----------------------|-----------------------|-------------------------------|-------------------------|
| 1                    | Conv2D               | 32 filtrÃ³w, 3Ã—3, ReLU         | (64, 64, 32)            |
| 2                    | BatchNormalization   | -                             | (64, 64, 32)            |
| 3                    | MaxPooling2D         | 2Ã—2                           | (32, 32, 32)            |
| 4                    | Dropout              | 0.25                          | (32, 32, 32)            |
| 5                    | Conv2D               | 64 filtrÃ³w, 3Ã—3, ReLU         | (32, 32, 64)            |
| 6                    | BatchNormalization   | -                             | (32, 32, 64)            |
| 7                    | MaxPooling2D         | 2Ã—2                           | (16, 16, 64)            |
| 8                    | Dropout              | 0.25                          | (16, 16, 64)            |
| 9                    | Conv2D               | 128 filtrÃ³w, 3Ã—3, ReLU        | (16, 16, 128)           |
| 10                   | BatchNormalization   | -                             | (16, 16, 128)           |
| 11                   | MaxPooling2D         | 2Ã—2                           | (8, 8, 128)             |
| 12                   | Dropout              | 0.4                           | (8, 8, 128)             |
| 13                   | Flatten              | -                             | (8192)                  |
| 14                   | Dense                | 256 neuronÃ³w, ReLU            | (256)                   |
| 15                   | Dropout              | 0.6                           | (256)                   |
| 16                   | Dense (Output)       | softmax, liczba klas = 6      | (6)                     |


     callbacks = [
        EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6),
        PrintEpochMetrics()
    ]


### a) UÅ¼ycie danych z kaggle - dataset z gestami dÅ‚oni i wytrenowanie na nim CNN
Rozmiar datasetu: 10 000 zdjÄ™Ä‡ (zostawiÅ‚am wybrane gesty, poÅ‚owÄ™ typu "C" usunÄ™Å‚am)

Model bardzo szybko osiÄ…ga val accuracy ponad 90 % (juÅ¼ w 3/4 epoce). SÄ… to bardzo niepokojÄ…ce wyniki, moÅ¼na zobaczyÄ‡ historiÄ™ uczenia na wykresie, gdzie widaÄ‡ niepokojÄ…co dobrÄ… dokÅ‚adnoÅ›Ä‡ modelu juÅ¼ w poczÄ…tkowych epokach. Model w teorii nie powinien popeÅ‚niaÄ‡ Å¼adnych bÅ‚Ä™dÃ³w, natomiast przy faktycznych testach z pomocÄ… pliku ./tests/CNN_test.py model (cnn_kaggle_model.keras) radzi sobie tragicznie, na 20 gestÃ³w rozpoznaÅ‚ jedynie 2 poprawnie, prawie bezbÅ‚Ä™dnie rozpoznaje jedynie gest "palm", reszty gestÃ³w nie rozpoznaje/myli je.
![Wyniki uczenia CNN na bazie danych kaggle](./classifiers/CNN/cnn_evaluation._kaggle.png)

### b) Trenowanie modelu z poÅ‚Ä…czonym dataset - gesty z kaggle oraz wÅ‚asne, stworzone za pomocÄ… pliku creating_photos.py
Rozmiar datasetu: 10 000 zdjÄ™c kaggle + 2042 wÅ‚asnych zdjÄ™c

Tutaj rÃ³wnieÅ¼ model bardzo szybko osiÄ…ga podejrzanie wysokie accuracy juÅ¼ w 3 epoce, macierz pomyÅ‚ek wyglÄ…da lepiej (accuracy 89%), lecz w praktycznym teÅ›cie model (cnn_hybrid_model.keras) radzi sobie porÃ³wnywalnie Åºle jak poprzedni model, rozpoznaje jedynie gest palm, czasem ok i l, wogole nie rozpoznaje V, fist i thumb.
![Wyniki uczenia CNN na Å‚Ä…Ä‡zonym zbiorze danych](./classifiers/CNN/cnn_hybrid_results.png)

### c) Wytrenowanie modelu jedynie na wÅ‚asnych danych  
Rozmiar datasetu: 2042 wÅ‚asnych zdjÄ™Ä‡ (./dataset/), ten dataset jest o wiele mniejszy, gesty pozostajÄ… takie same

Tutaj widaÄ‡ proces uczenia siÄ™ nie byÅ‚ tak idealny jak w etapach a i b, ale wciÄ…Å¼ loss staje siÄ™ minimalnie niski juÅ¼ po 5 epoce, macierz pomyÅ‚ek z pozoru wyglÄ…da bardzo dobrze, accuracy 98%
Praktyczny test wykazuje jednak, Å¼e model radzi sobie w 10/20 gestÃ³w testowych, rozpoznaje thumb, palm, l oraz ok, natomiast V i fist nie jest w stanie rozpoznaÄ‡ wogole.
![Wyniki uczenia CNN na wÅ‚asnych danych](./classifiers/CNN/cnn_custom_only_results.png)

Wnioski - zestaw zdjÄ™c z datasetu kaggle jest sÅ‚aby, model uczony na nim zachowuje siÄ™ jakby wystÄ™powaÅ‚ leakage (byÄ‡ moÅ¼e zdjÄ™cia sÄ… zbyt podobne do siebie). Najlepiej radzi sobie model trenowany na wÅ‚asnych danych, jednakÅ¼e niestety wciÄ…Å¼ widoczne jest przeuczenie.


## Etap trzeci - transfer learning
ZROB ANALOGICZNY OPIS TEGO PREPROCESSINGU

Transfer learning - douczenie gotowego modelu MobileNetV2 z biblioteki keras na wÅ‚asnych danych - czyli na 6 gestach V, fist, palm, thumb, ok, l 

Dwufazowe podejÅ›cie do trenowania:
- **Faza 1**: Feature extraction - zamroÅ¼one warstwy bazowe MobileNetV2, trenowanie tylko klasyfikatora
- **Faza 2**: Fine-tuning - odmroÅ¼enie ostatnich 50 warstw i douczanie z bardzo niskim learning rate (0.00001)

Architektura Klasyfikatora:
| Warstwa                       | Typ                     | Parametry                              | WyjÅ›cie (Output Shape) |
|-------------------------------|--------------------------|------------------------------------------|-------------------------|
| 1                             | MobileNetV2 (bazowa)     | pretrained, zamroÅ¼ona (czÄ™Å›ciowo)        | (7, 7, 1280)            |
| 2                             | GlobalAveragePooling2D   | -                                        | (1280)                  |
| 3                             | BatchNormalization       | -                                        | (1280)                  |
| 4                             | Dense                    | 256 neuronÃ³w, ReLU                       | (256)                   |
| 5                             | Dropout                  | 0.5 lub 0.6                              | (256)                   |
| 6                             | Dense                    | 128 neuronÃ³w, ReLU                       | (128)                   |
| 7                             | Dropout                  | 0.5 lub 0.6                              | (128)                   |
| 8                             | Dense (Output)           | 6 neuronÃ³w, softmax (6 klas)             | (6)                     |

Dane przetwarzane do rozmiaru 224Ã—224 (wymagany przez MobileNetV2).
Augmentacja danych w fazie trenowania:
- Rotacja: Â±15Â°
- PrzesuniÄ™cia: Â±10%  
- Zoom: Â±10%
- Zmiana jasnoÅ›ci: 80-120%
- Brak horizontal_flip (nie wszystkie gesty sÄ… symetryczne np. thumb)

Jak widac na wykresie model w naturalny sposÃ³b osiÄ…ga wysokie accuracy i niski loss, brak widocznego przeuczenia. Na macierzy pomyÅ‚ek widaÄ‡, Å¼e test accuracy jest prawie caÅ‚kowita. Test praktyczny daje wyniki gorsze niÅ¼ oczekiwane ale lepsze niÅ¼ wÅ‚asny CNN, model wykrywa 15/20 gestÃ³w poprawnie, najwiÄ™kszy problem ma z gestem ok.
![Wyniku uczenia MobileNetV2](./classifiers/MobileNetV2/mobilenetv2_results.png)



## Etap czwarty - powiÄ™kszanie wÅ‚asnego datasetu i doszkalanie najlepszych klasyfikatorÃ³w 

Wybrano po jednym klasyfikatorze z dwÃ³ch etapÃ³w, ktÃ³re najlepiej zdaÅ‚y test praktyczny 
Etap I - Decision Tree Classifier 
Etap III - MobileNetV2

Do datasetu wspÃ³Å‚rzÄ™dnych gestÃ³w ./data_sources/gestures.csv dodano 3 nowe gesty: 'thumb', 'fist', 'rock', kaÅ¼dy liczÄ…cy po ok 450 zebranych danych, tak jak reszta poprzednich gestÃ³w.
Dataset zdjÄ™Ä‡ ./dataset/ powiÄ™kszono z 400 zdjÄ™Ä‡ na kaÅ¼dy gest do 650 zdjÄ™Ä‡ oraz dodano nowy gest 'rock' liczÄ…cy rÃ³wnieÅ¼ 650 zdjÄ™Ä‡.


Decision Tree Classifier
Accuracy: 0.9917184265010351
Cross-validation scores: [0.97670807 0.96583851 0.99688958 0.98911353 0.93623639]
Classification Report:
              precision    recall  f1-score   support

           L       0.99      1.00      0.99       149
           V       1.00      0.97      0.98       121
        fist       1.00      0.99      1.00       134
          ok       1.00      0.98      0.99       122
        palm       0.99      1.00      1.00       161
        rock       0.97      1.00      0.98       150
       thumb       1.00      1.00      1.00       129

    accuracy                           0.99       966
   macro avg       0.99      0.99      0.99       966
weighted avg       0.99      0.99      0.99       966

W teÅ›cie praktycznym klasyfikator radzi sobie doskonale, na 20 prÃ³b wykrywa wszystkie gesty poprawnie, jedyna drobna uwaga to czasem ma problem z 'palm' i z niewiadomego powodu klasyfikuje to jako 'ok', po poruszeniu/przybliÅ¼eniu rÄ™ki jednak klasyfikator identyfikuje gest dobrze.

![Macierz pomyÅ‚ek szkolenia DTC na rozszerzoym datasecie](./classifiers/DTC/dtc_extended_confusion_matrix.png)











lstm
ğŸ“‹ Architektura modelu:
Model: "sequential"
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Layer (type)                         â”ƒ Output Shape                â”ƒ         Param # â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ lstm_1 (LSTM)                        â”‚ (None, 10, 128)             â”‚          87,552 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ batch_normalization                  â”‚ (None, 10, 128)             â”‚             512 â”‚
â”‚ (BatchNormalization)                 â”‚                             â”‚                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout (Dropout)                    â”‚ (None, 10, 128)             â”‚               0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ lstm_2 (LSTM)                        â”‚ (None, 10, 64)              â”‚          49,408 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ batch_normalization_1                â”‚ (None, 10, 64)              â”‚             256 â”‚
â”‚ (BatchNormalization)                 â”‚                             â”‚                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout_1 (Dropout)                  â”‚ (None, 10, 64)              â”‚               0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ lstm_3 (LSTM)                        â”‚ (None, 32)                  â”‚          12,416 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ batch_normalization_2                â”‚ (None, 32)                  â”‚             128 â”‚
â”‚ (BatchNormalization)                 â”‚                             â”‚                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout_2 (Dropout)                  â”‚ (None, 32)                  â”‚               0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dense_1 (Dense)                      â”‚ (None, 64)                  â”‚           2,112 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout_3 (Dropout)                  â”‚ (None, 64)                  â”‚               0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dense_2 (Dense)                      â”‚ (None, 32)                  â”‚           2,080 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout_4 (Dropout)                  â”‚ (None, 32)                  â”‚               0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ output (Dense)                       â”‚ (None, 4)                   â”‚             132 